#%% load toolboxes
## general package
import numpy as np
import matplotlib.pyplot as plt 

## package for extracting audio features
import resampy
import fairseq
from scipy.io import wavfile

## package for extracting visual features
import cv2
import torch
import torchvision
from torch import nn
from torchvision import transforms


#%% Define some func.
def load_video(video_path):
    v = cv2.VideoCapture(video_path)
    # check if video is opened successfully
    if not v.isOpened():
        print('Error opening the file.')
        exit(0)
    return v

def video_info(video):
    NFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    Video_FPS = cap.get(cv2.CAP_PROP_FPS)
    Video_Res = [cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT)]
    print('Video dimensions:', Video_Res)
    print('Total frame number:', NFrames)
    print('Video FPS (Frame Per Second):', Video_FPS)
    return NFrames, Video_FPS, Video_Res


#%% Device agnostic code
''' for google colab '''
# device = 'cuda' if torch.cuda.is_available() else 'cpu'  

''' for mac '''
if torch.backends.mps.is_available() & torch.backends.mps.is_built():
    device = 'mps'
else:
    device = 'cpu'








## Load the audio
orig_srate, wav = wavfile.read('Data/Foreign_Language_trigger.wav')
print('the original sample rate of the audio is {} Hz.'.format(orig_srate))

# downsample the audio to 16000Hz
new_srate = 16000
wav = wav[:, 1]
wav = resampy.resample(wav, orig_srate, new_srate)


## load wav2vec_base model
checkpoint = torch.load('Data/wav2vec_model/wav2vec_small_NFT.pt')
cfg = fairseq.dataclass.utils.convert_namespace_to_omegaconf(checkpoint['args'])
aud_model = fairseq.models.wav2vec.Wav2Vec2Model.build_model(cfg.model)
aud_model.load_state_dict(checkpoint['model'])
# aud_model.eval()


## Feature generation
# input preprocessing
wav = wav[np.newaxis, :]
wav = torch.from_numpy(wav)

# generate transformer-block features
# Here I followed the pipeline of the original paper:
#   - Batches are built by cropping 250k audio samples, or 15.6sec, from each example.
N = 250000
seg_num = int((N - 400) / 320 + 1)
seg_N = np.floor(wav.shape[1] / N)
trs_7 = np.zeros((int(seg_num * seg_N + 644), 768))

for n in range(int(seg_N + 1)): 
    print('current seg number is {}'.format(n))
    if n == int(seg_N):
        wav_seg = wav[:, N * n :]; trs = []
        while len(trs) != 12:
            trs = aud_model.extract_features(source = wav_seg, padding_mask = None)
            trs = trs['layer_results']
        print(len(trs))
        trs_7[seg_num * n :, :] = np.squeeze(trs[6][0].detach().numpy())
    else:
        wav_seg = wav[:, N * n : N * (n + 1)]; trs = []
        while len(trs) != 12:  # avoid the layer dropout
            trs = aud_model.extract_features(source = wav_seg, padding_mask = None)
            trs = trs['layer_results']
        trs_7[seg_num * n : seg_num * (n + 1), :] = np.squeeze(trs[6][0].detach().numpy())


## visualize features
plt.figure(figsize = (10, 3))
plt.imshow(trs_7.transpose(), vmin = -0.5, vmax = 0.5, aspect = 'auto')
plt.xlabel('time (samples)')
plt.ylabel('features')
plt.show()














## load video
cap = load_video('Data/Foreign_Language_trigger.mp4')

## video info
[NFrames, Video_FPS, Video_Res] = video_info(cap)


## load ViT model: ViT huge model (32*32 patch)
ViT_model = torchvision.models.vit_h_14(weights = 'IMAGENET1K_SWAG_E2E_V1', feature_extraction_mode = 1)
# ViT_model.state_dict()  # model info
# ViT_model.state_dict().keys()  # layer info


## Feature generation
miu = torch.reshape(torch.tensor([0.485, 0.456, 0.406]), (3, 1, 1))  # used for normalization
sigma = torch.reshape(torch.tensor([0.229, 0.224, 0.225]), (3, 1, 1))  # used for normalization

## movie data preprocessing
vitH16 = np.zeros((NFrames, 1280))

for f in range(NFrames):
    print('current frame is {}'.format(f))
    cap.set(cv2.CAP_PROP_POS_FRAMES, f)
    ret, img = cap.read()
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # [H, W, C]
    img = cv2.resize(img, (518, 518), interpolation = cv2.INTER_AREA)  # resizing to [224, 224]
    img = img / 255  # rescale to [0, 1]
    img = torch.tensor(img).permute(2, 0, 1)  # [C, H, W]
    img = (img - miu) / sigma
    img = img[np.newaxis, :, : , :]
    features = ViT_model.my_forward(img.to(torch.float32))
    vitH16[f, :] = features[15].detach().numpy()
    del features


## visualize features
plt.figure(figsize = (10, 3))
plt.imshow(vitH16.transpose(), vmin = -0.5, vmax = 0.5, aspect = 'auto')
plt.xlabel('time (samples)')
plt.ylabel('features')
plt.show()



